# Enhanced Smart Parser with PyThaiNLP
# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö EnhancedSmartDateTimeParser

from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
import re

# PyThaiNLP imports (‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô)
"""
from pythainlp import word_tokenize
from pythainlp.tag import pos_tag, named_entity
from pythainlp.util import normalize
from pythainlp.corpus import thai_stopwords
"""

class EnhancedSmartDateTimeParser:
    """
    Enhanced version ‡∏Ç‡∏≠‡∏á SmartDateTimeParser ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ PyThaiNLP
    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏â‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
    """
    
    def __init__(self):
        self.setup_pythainlp_components()
        self.setup_patterns()
        self.setup_thai_context_maps()
    
    def setup_pythainlp_components(self):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ PyThaiNLP components"""
        # ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏à‡∏∞‡πÉ‡∏™‡πà PyThaiNLP ‡∏à‡∏£‡∏¥‡∏á
        self.use_pythainlp = False  # Flag ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î PyThaiNLP
        
        if self.use_pythainlp:
            try:
                # import ‡πÅ‡∏•‡∏∞ setup PyThaiNLP
                pass
            except ImportError:
                print("PyThaiNLP not available, using basic parsing")
                self.use_pythainlp = False
    
    def setup_patterns(self):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ patterns ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ parse ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô"""
        
        # Command patterns for detecting appointment requests
        self.command_patterns = {
            'add_appointment': [
                r'‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ô‡∏±‡∏î',
                r'‡∏ô‡∏±‡∏î‡∏´‡∏°‡∏≤‡∏¢',
                r'‡∏à‡∏≠‡∏á‡∏ô‡∏±‡∏î',
                r'‡∏ï‡∏±‡πâ‡∏á‡∏ô‡∏±‡∏î',
                r'‡∏Ç‡∏≠‡∏ô‡∏±‡∏î'
            ]
        }

        # Date patterns (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô)
        self.date_patterns = {
            'relative_day': {
                '‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ': 0,
                '‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ': 1,
                '‡∏°‡∏∞‡∏£‡∏∑‡∏ô‡∏ô‡∏µ‡πâ': 2,
                '‡∏°‡∏∞‡∏£‡∏∑‡∏ô': 2,
                '‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô': -1,
                '‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô‡∏ô‡∏µ‡πâ': -1,
                '‡∏°‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô': -2
            },
            'weekday': {
                '‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå': 0, '‡∏≠‡∏±‡∏á‡∏Ñ‡∏≤‡∏£': 1, '‡∏û‡∏∏‡∏ò': 2,
                '‡∏û‡∏§‡∏´‡∏±‡∏™‡∏ö‡∏î‡∏µ': 3, '‡∏û‡∏§‡∏´‡∏±‡∏™': 3, '‡∏®‡∏∏‡∏Å‡∏£‡πå': 4,
                '‡πÄ‡∏™‡∏≤‡∏£‡πå': 5, '‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå': 6
            },
            'weekday_modifier': {
                '‡∏ô‡∏µ‡πâ': 0, '‡∏´‡∏ô‡πâ‡∏≤': 1, '‡∏ñ‡∏±‡∏î‡πÑ‡∏õ': 1, '‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß': -1
            },
            'period': [
                r'‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏´‡∏ô‡πâ‡∏≤',
                r'‡∏™‡∏¥‡πâ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',
                r'‡∏ï‡πâ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',
                r'‡∏õ‡∏•‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',
                r'‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤',
                r'‡∏õ‡∏µ‡∏´‡∏ô‡πâ‡∏≤'
            ]
        }

        # Time patterns (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô)
        self.time_patterns = {
            'formal': [
                r'(?P<h>\d{1,2})[:.](?P<m>\d{2})\s*‡∏ô?\.?',
                r'‡πÄ‡∏ß‡∏•‡∏≤\s*(?P<h>\d{1,2})[:.](?P<m>\d{2})',
                r'(?P<h>\d{1,2})\s*‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤\s*(?P<m>\d{2})?'
            ],
            'spoken': [
                r'(?P<h>\d{1,2})\s*‡πÇ‡∏°‡∏á(?P<period>‡πÄ‡∏ä‡πâ‡∏≤|‡πÄ‡∏¢‡πá‡∏ô|‡∏ï‡∏£‡∏á)?(?P<half>‡∏Ñ‡∏£‡∏∂‡πà‡∏á)?',
                r'‡∏ö‡πà‡∏≤‡∏¢\s*(?P<h>\d{1,2})(?P<half>‡∏Ñ‡∏£‡∏∂‡πà‡∏á)?',
                r'(?P<h>\d{1,2})\s*‡πÇ‡∏°‡∏á‡πÄ‡∏¢‡πá‡∏ô(?P<half>‡∏Ñ‡∏£‡∏∂‡πà‡∏á)?',
                r'(?P<h>\d{1,2})\s*‡∏ó‡∏∏‡πà‡∏°(?P<half>‡∏Ñ‡∏£‡∏∂‡πà‡∏á)?',
                r'‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏á‡∏ï‡∏£‡∏á',
                r'‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏á‡∏Ñ‡∏∑‡∏ô',
                r'‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏á',
                r'‡∏ï‡∏≠‡∏ô(‡πÄ‡∏ä‡πâ‡∏≤|‡∏™‡∏≤‡∏¢|‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏á|‡∏ö‡πà‡∏≤‡∏¢|‡πÄ‡∏¢‡πá‡∏ô|‡∏Ñ‡πà‡∏≥|‡∏î‡∏∂‡∏Å)'
            ],
            'period_defaults': {
                '‡πÄ‡∏ä‡πâ‡∏≤': (6, 11),
                '‡∏™‡∏≤‡∏¢': (9, 11),
                '‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏á': (11, 13),
                '‡∏ö‡πà‡∏≤‡∏¢': (12, 17),
                '‡πÄ‡∏¢‡πá‡∏ô': (17, 19),
                '‡∏Ñ‡πà‡∏≥': (18, 21),
                '‡∏î‡∏∂‡∏Å': (21, 24)
            }
        }

        # Location patterns (‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô)
        self.location_patterns = {
            'connectors': [
                r'‡∏ó‡∏µ‡πà\s+(.+?)(?:\s|$|[,.])',
                r'‡∏ì\s+(.+?)(?:\s|$|[,.])',
                r'‡πÉ‡∏ô\s+(.+?)(?:\s|$|[,.])',
                r'‡πÑ‡∏õ\s*(.+?)(?:\s|$|[,.])'
            ],
            'specific': [
                r'‡∏ó‡∏µ‡πà‡∏™‡∏ô‡∏≤‡∏°‡∏ö‡∏¥‡∏ô(.+?)(?:\s|$|[,.])',
                r'‡∏™‡∏ô‡∏≤‡∏°‡∏ö‡∏¥‡∏ô(.+?)(?:\s|$|[,.])',
                r'‡∏ó‡∏µ‡πà‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•(.+?)(?:\s|$|[,.])',
                r'‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•(.+?)(?:\s|$|[,.])',
                r'‡∏£‡∏û\.?\s*(.+?)(?:\s|$|[,.])',
                r'‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å(.+?)(?:\s|$|[,.])'
            ]
        }

        # (Moved normalization_maps and later context maps to setup_thai_context_maps)
        return

        # Time mapping rules (UNREACHABLE - kept for reference, real init in setup_thai_context_maps)
        self.time_mappings = {
            # Hour conversion rules
            'hour_conversions': {
                '‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏á‡∏ï‡∏£‡∏á': (12, 0),
                '‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏á‡∏Ñ‡∏∑‡∏ô': (0, 0),
                '‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏á': (12, 0)
            },
            
            # Period-specific hour adjustments
            'period_adjustments': {
                '‡πÄ‡∏ä‡πâ‡∏≤': lambda h: h if h >= 6 else h + 12 if h <= 5 else h,
                '‡∏™‡∏≤‡∏¢': lambda h: h if h >= 9 else h + 12,
                '‡∏ö‡πà‡∏≤‡∏¢': lambda h: h + 12 if h <= 6 else h,
                '‡πÄ‡∏¢‡πá‡∏ô': lambda h: h + 12 if h <= 7 else h,
                '‡∏ó‡∏∏‡πà‡∏°': lambda h: h + 18,  # 1 ‡∏ó‡∏∏‡πà‡∏° = 19:00, 2 ‡∏ó‡∏∏‡πà‡∏° = 20:00
            },
            
            # Common time expressions
            'expressions': {
                '‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ó‡∏∏‡πà‡∏°': (19, 0),
                '‡∏™‡∏≠‡∏á‡∏ó‡∏∏‡πà‡∏°': (20, 0),
                '‡∏™‡∏≤‡∏°‡∏ó‡∏∏‡πà‡∏°': (21, 0),
                '‡∏™‡∏µ‡πà‡∏ó‡∏∏‡πà‡∏°': (22, 0),
                '‡∏´‡πâ‡∏≤‡∏ó‡∏∏‡πà‡∏°': (23, 0),
                '‡∏´‡∏Å‡∏ó‡∏∏‡πà‡∏°': (0, 0),  # Next day
                '‡∏ö‡πà‡∏≤‡∏¢‡∏™‡∏≠‡∏á': (14, 0),
                '‡∏ö‡πà‡∏≤‡∏¢‡∏™‡∏≠‡∏á‡∏Ñ‡∏£‡∏∂‡πà‡∏á': (14, 30),
                '‡∏™‡∏≤‡∏°‡∏ó‡∏∏‡πà‡∏°‡∏Ñ‡∏£‡∏∂‡πà‡∏á': (21, 30),
                '‡∏™‡∏µ‡πà‡πÇ‡∏°‡∏á‡πÄ‡∏¢‡πá‡∏ô': (16, 0)
            }
        }
        
        # Medical context keywords (refined)
        self.medical_keywords = {
            'doctor': ['‡∏´‡∏°‡∏≠', '‡πÅ‡∏û‡∏ó‡∏¢‡πå', '‡∏î‡∏£.', '‡∏ô‡∏û.', '‡∏û‡∏ç.'],
            'appointment': ['‡∏ï‡∏£‡∏ß‡∏à', '‡∏£‡∏±‡∏Å‡∏©‡∏≤', '‡∏â‡∏µ‡∏î'],  # Removed generic words
            'hospital': ['‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•', '‡∏£‡∏û.', '‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å', '‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå'],
            'medical_action': ['‡∏ï‡∏£‡∏ß‡∏à', '‡∏£‡∏±‡∏Å‡∏©‡∏≤', '‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î', '‡∏â‡∏µ‡∏î‡∏¢‡∏≤', '‡πÄ‡∏≠‡πá‡∏Å‡∏ã‡πÄ‡∏£‡∏¢‡πå', '‡∏Ñ‡∏£‡∏±‡πà‡∏≤‡∏á', '‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå']
        }
        
        # Business context keywords  
        self.business_keywords = {
            'meeting': ['‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°', '‡∏°‡∏µ‡∏ï‡∏¥‡∏á', '‡∏û‡∏ö', '‡∏´‡∏≤‡∏£‡∏∑‡∏≠', '‡∏™‡∏±‡∏°‡∏°‡∏ô‡∏≤'],
            'location': ['‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏≠‡∏≠‡∏ü‡∏ü‡∏¥‡∏®', '‡∏≠‡∏≠‡∏ü‡∏ü‡∏¥‡∏®'],
            'formal': ['‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°', '‡∏Å‡∏≤‡∏£‡∏û‡∏ö‡∏õ‡∏∞', '‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏£‡∏∑‡∏≠']
        }
        
        # Date calculation helpers
        self.date_helpers = {
            'weekday_names': ['‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå', '‡∏≠‡∏±‡∏á‡∏Ñ‡∏≤‡∏£', '‡∏û‡∏∏‡∏ò', '‡∏û‡∏§‡∏´‡∏±‡∏™‡∏ö‡∏î‡∏µ', '‡∏û‡∏§‡∏´‡∏±‡∏™', '‡∏®‡∏∏‡∏Å‡∏£‡πå', '‡πÄ‡∏™‡∏≤‡∏£‡πå', '‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå'],
            'period_calculations': {
                '‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏´‡∏ô‡πâ‡∏≤': lambda date: date + timedelta(weeks=1),
                '‡∏™‡∏¥‡πâ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô': lambda date: self._end_of_month(date),
                '‡∏ï‡πâ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô': lambda date: self._start_of_month(date), 
                '‡∏õ‡∏•‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô': lambda date: self._end_of_month(date),
                '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤': lambda date: self._next_month(date)
            }
        }

        # === THAI_MONTHS_MAP (Integrated 2025-10-08) ===
        self.thai_months: Dict[str, int] = {
            '‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°': 1, '‡∏°.‡∏Ñ.': 1, '‡∏°‡∏Ñ': 1, '‡∏°‡∏Ñ.': 1,
            '‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå': 2, '‡∏Å.‡∏û.': 2, '‡∏Å‡∏û': 2, '‡∏Å‡∏û.': 2,
            '‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°': 3, '‡∏°‡∏µ.‡∏Ñ.': 3, '‡∏°‡∏µ‡∏Ñ': 3, '‡∏°‡∏µ‡∏Ñ.': 3,
            '‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô': 4, '‡πÄ‡∏°.‡∏¢.': 4, '‡πÄ‡∏°‡∏¢': 4, '‡πÄ‡∏°‡∏¢.': 4,
            '‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°': 5, '‡∏û.‡∏Ñ.': 5, '‡∏û‡∏Ñ': 5, '‡∏û‡∏Ñ.': 5,
            '‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô': 6, '‡∏°‡∏¥.‡∏¢.': 6, '‡∏°‡∏¥‡∏¢': 6, '‡∏°‡∏¥‡∏¢.': 6,
            '‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°': 7, '‡∏Å.‡∏Ñ.': 7, '‡∏Å‡∏Ñ': 7, '‡∏Å‡∏Ñ.': 7,
            '‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°': 8, '‡∏™.‡∏Ñ.': 8, '‡∏™‡∏Ñ': 8, '‡∏™‡∏Ñ.': 8,
            '‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô': 9, '‡∏Å.‡∏¢.': 9, '‡∏Å‡∏¢': 9, '‡∏Å‡∏¢.': 9,
            '‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°': 10, '‡∏ï.‡∏Ñ.': 10, '‡∏ï‡∏Ñ': 10, '‡∏ï‡∏Ñ.': 10,
            '‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô': 11, '‡∏û.‡∏¢.': 11, '‡∏û‡∏¢': 11, '‡∏û‡∏¢.': 11,
            '‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°': 12, '‡∏ò.‡∏Ñ.': 12, '‡∏ò‡∏Ñ': 12, '‡∏ò‡∏Ñ.': 12
        }
    
    def setup_thai_context_maps(self):
        """Setup Thai-specific normalization maps, time mappings, domain keywords, helpers, and month names.

        This method was missing (causing AttributeError). It consolidates context
        data that earlier patches moved out of setup_patterns. Keep definitions
        minimal but sufficient for current parser logic.
        """
        # 1. Normalization maps used in pre_normalize_text
        self.normalization_maps = {
            'thai_numerals': {
                '‡πê': '0', '‡πë': '1', '‡πí': '2', '‡πì': '3', '‡πî': '4',
                '‡πï': '5', '‡πñ': '6', '‡πó': '7', '‡πò': '8', '‡πô': '9'
            },
            'abbreviations': {
                '‡∏û‡∏ô.': '‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ',
                '‡∏°‡∏∞‡∏£‡∏∑‡∏ô': '‡∏°‡∏∞‡∏£‡∏∑‡∏ô‡∏ô‡∏µ‡πâ',
                '‡∏£‡∏û.': '‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•',
                '‡∏£‡∏û': '‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•'
            },
            'location_aliases': {
                '‡∏£‡∏û.': '‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•',
                '‡∏£‡∏û': '‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•',
                '‡∏£‡∏£.': '‡πÇ‡∏£‡∏á‡πÅ‡∏£‡∏°'
            }
        }

        # 2. Time mappings (used by extract_time_patterns / parse_spoken_time)
        self.time_mappings = {
            'hour_conversions': {
                '‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏á‡∏ï‡∏£‡∏á': (12, 0),
                '‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏á‡∏Ñ‡∏∑‡∏ô': (0, 0),
                '‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏á': (12, 0)
            },
            'period_adjustments': {
                '‡πÄ‡∏ä‡πâ‡∏≤': lambda h: h if h >= 6 else h + 12 if h <= 5 else h,
                '‡∏™‡∏≤‡∏¢': lambda h: h if h >= 9 else h + 12,
                '‡∏ö‡πà‡∏≤‡∏¢': lambda h: h + 12 if h <= 6 else h,
                '‡πÄ‡∏¢‡πá‡∏ô': lambda h: h + 12 if h <= 7 else h,
                '‡∏ó‡∏∏‡πà‡∏°': lambda h: h + 18,  # 1 ‡∏ó‡∏∏‡πà‡∏° = 19:00
            },
            'expressions': {
                '‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ó‡∏∏‡πà‡∏°': (19, 0),
                '‡∏™‡∏≠‡∏á‡∏ó‡∏∏‡πà‡∏°': (20, 0),
                '‡∏™‡∏≤‡∏°‡∏ó‡∏∏‡πà‡∏°': (21, 0),
                '‡∏™‡∏µ‡πà‡∏ó‡∏∏‡πà‡∏°': (22, 0),
                '‡∏´‡πâ‡∏≤‡∏ó‡∏∏‡πà‡∏°': (23, 0),
                '‡∏´‡∏Å‡∏ó‡∏∏‡πà‡∏°': (0, 0),
                '‡∏ö‡πà‡∏≤‡∏¢‡∏™‡∏≠‡∏á': (14, 0),
                '‡∏ö‡πà‡∏≤‡∏¢‡∏™‡∏≠‡∏á‡∏Ñ‡∏£‡∏∂‡πà‡∏á': (14, 30),
                '‡∏™‡∏≤‡∏°‡∏ó‡∏∏‡πà‡∏°‡∏Ñ‡∏£‡∏∂‡πà‡∏á': (21, 30),
                '‡∏™‡∏µ‡πà‡πÇ‡∏°‡∏á‡πÄ‡∏¢‡πá‡∏ô': (16, 0)
            }
        }

        # 3. Domain keywords
        self.medical_keywords = {
            'doctor': ['‡∏´‡∏°‡∏≠', '‡πÅ‡∏û‡∏ó‡∏¢‡πå', '‡∏î‡∏£.', '‡∏ô‡∏û.', '‡∏û‡∏ç.'],
            'appointment': ['‡∏ï‡∏£‡∏ß‡∏à', '‡∏£‡∏±‡∏Å‡∏©‡∏≤', '‡∏â‡∏µ‡∏î'],
            'hospital': ['‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•', '‡∏£‡∏û.', '‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å', '‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå'],
            'medical_action': ['‡∏ï‡∏£‡∏ß‡∏à', '‡∏£‡∏±‡∏Å‡∏©‡∏≤', '‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î', '‡∏â‡∏µ‡∏î‡∏¢‡∏≤', '‡πÄ‡∏≠‡πá‡∏Å‡∏ã‡πÄ‡∏£‡∏¢‡πå', '‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå']
        }
        self.business_keywords = {
            'meeting': ['‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°', '‡∏°‡∏µ‡∏ï‡∏¥‡∏á', '‡∏û‡∏ö', '‡∏´‡∏≤‡∏£‡∏∑‡∏≠', '‡∏™‡∏±‡∏°‡∏°‡∏ô‡∏≤'],
            'location': ['‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏≠‡∏≠‡∏ü‡∏ü‡∏¥‡∏®'],
            'formal': ['‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°', '‡∏Å‡∏≤‡∏£‡∏û‡∏ö‡∏õ‡∏∞', '‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏£‡∏∑‡∏≠']
        }

        # 4. Date helpers (for potential future use)
        self.date_helpers = {
            'weekday_names': ['‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå', '‡∏≠‡∏±‡∏á‡∏Ñ‡∏≤‡∏£', '‡∏û‡∏∏‡∏ò', '‡∏û‡∏§‡∏´‡∏±‡∏™‡∏ö‡∏î‡∏µ', '‡∏û‡∏§‡∏´‡∏±‡∏™', '‡∏®‡∏∏‡∏Å‡∏£‡πå', '‡πÄ‡∏™‡∏≤‡∏£‡πå', '‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå'],
            'period_calculations': {
                '‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏´‡∏ô‡πâ‡∏≤': lambda date: date + timedelta(weeks=1),
                '‡∏™‡∏¥‡πâ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô': lambda date: self._end_of_month(date),
                '‡∏ï‡πâ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô': lambda date: self._start_of_month(date),
                '‡∏õ‡∏•‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô': lambda date: self._end_of_month(date),
                '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤': lambda date: self._next_month(date)
            }
        }

        # 5. Thai month names (ensure available before date extraction)
        self.thai_months = {
            '‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°': 1, '‡∏°.‡∏Ñ.': 1, '‡∏°‡∏Ñ': 1, '‡∏°‡∏Ñ.': 1,
            '‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå': 2, '‡∏Å.‡∏û.': 2, '‡∏Å‡∏û': 2, '‡∏Å‡∏û.': 2,
            '‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°': 3, '‡∏°‡∏µ.‡∏Ñ.': 3, '‡∏°‡∏µ‡∏Ñ': 3, '‡∏°‡∏µ‡∏Ñ.': 3,
            '‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô': 4, '‡πÄ‡∏°.‡∏¢.': 4, '‡πÄ‡∏°‡∏¢': 4, '‡πÄ‡∏°‡∏¢.': 4,
            '‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°': 5, '‡∏û.‡∏Ñ.': 5, '‡∏û‡∏Ñ': 5, '‡∏û‡∏Ñ.': 5,
            '‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô': 6, '‡∏°‡∏¥.‡∏¢.': 6, '‡∏°‡∏¥‡∏¢': 6, '‡∏°‡∏¥‡∏¢.': 6,
            '‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°': 7, '‡∏Å.‡∏Ñ.': 7, '‡∏Å‡∏Ñ': 7, '‡∏Å‡∏Ñ.': 7,
            '‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°': 8, '‡∏™.‡∏Ñ.': 8, '‡∏™‡∏Ñ': 8, '‡∏™‡∏Ñ.': 8,
            '‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô': 9, '‡∏Å.‡∏¢.': 9, '‡∏Å‡∏¢': 9, '‡∏Å‡∏¢.': 9,
            '‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°': 10, '‡∏ï.‡∏Ñ.': 10, '‡∏ï‡∏Ñ': 10, '‡∏ï‡∏Ñ.': 10,
            '‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô': 11, '‡∏û.‡∏¢.': 11, '‡∏û‡∏¢': 11, '‡∏û‡∏¢.': 11,
            '‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°': 12, '‡∏ò.‡∏Ñ.': 12, '‡∏ò‡∏Ñ': 12, '‡∏ò‡∏Ñ.': 12,
            # Informal / spoken short forms
            '‡∏°‡∏Å‡∏£‡∏≤': 1, '‡∏Å‡∏∏‡∏°‡∏†‡∏≤': 2, '‡∏°‡∏µ‡∏ô‡∏≤': 3, '‡πÄ‡∏°‡∏©‡∏≤': 4,
            '‡∏û‡∏§‡∏©‡∏†‡∏≤': 5, '‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤': 6, '‡∏Å‡∏£‡∏Å‡∏é‡∏≤': 7, '‡∏™‡∏¥‡∏á‡∏´‡∏≤': 8,
            '‡∏Å‡∏±‡∏ô‡∏¢‡∏≤': 9, '‡∏ï‡∏∏‡∏•‡∏≤': 10, '‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤': 11, '‡∏ò‡∏±‡∏ô‡∏ß‡∏≤': 12
        }

        # Optional: simple contact patterns container to avoid attribute errors if used
        self.contact_patterns = {
            'phone': [r'(0\d{2}-?\d{3}-?\d{4})']
        }

        # Debug marker
        # print('[setup_thai_context_maps] initialized')
    
    def preprocess_text_basic(self, text: str) -> Dict[str, Any]:
        """
        Basic text preprocessing with comprehensive normalization
        """
        # Step 1: Pre-normalize text
        normalized_text = self.pre_normalize_text(text)
        
        # Step 2: Basic tokenization (split by spaces for now)
        basic_tokens = self.tokenize_basic(normalized_text)
        
        # Step 3: Extract patterns using enhanced rules
        time_matches = self.extract_time_patterns(normalized_text)
        date_matches = self.extract_date_patterns(normalized_text)
        location_matches = self.extract_location_patterns(normalized_text)
        contact_matches = self.extract_contact_patterns(normalized_text)
        
        return {
            'original': text,
            'normalized': normalized_text,
            'tokens': basic_tokens,
            'time_matches': time_matches,
            'date_matches': date_matches,
            'location_matches': location_matches,
            'contact_matches': contact_matches,
            'context': self.detect_basic_context(normalized_text)
        }
    
    def pre_normalize_text(self, text: str) -> str:
        """Pre-normalize text according to specifications"""
        import re
        
        # 1. Remove excessive whitespace and unnecessary emojis
        text = re.sub(r'\s+', ' ', text.strip())
        text = re.sub(r'[üòÄ-üøø]+', '', text)  # Remove emojis
        
        # 2. Convert Thai numerals to Arabic
        for thai, arabic in self.normalization_maps['thai_numerals'].items():
            text = text.replace(thai, arabic)
        
        # 3. Replace abbreviations/slang with standard terms
        for abbr, standard in self.normalization_maps['abbreviations'].items():
            text = text.replace(abbr, standard)
        
        # 4. Apply location aliases (more careful approach)
        for alias, canonical in self.normalization_maps['location_aliases'].items():
            # Only replace if it's a standalone word, not part of another word
            import re
            pattern = r'\b' + re.escape(alias) + r'\b'
            text = re.sub(pattern, canonical, text)
        
        return text.strip()
    
    def tokenize_basic(self, text: str) -> List[str]:
        """Basic tokenization (enhanced when PyThaiNLP available)"""
        if self.use_pythainlp:
            # Will use PyThaiNLP tokenization when available
            try:
                from pythainlp import word_tokenize  # type: ignore[import-not-found]
                return word_tokenize(text, engine='newmm')
            except ImportError:
                pass
        
        # Fallback to simple tokenization
        import re
        # Split by spaces and punctuation but keep Thai words together
        tokens = re.findall(r'[\u0E00-\u0E7F]+|\d+|[a-zA-Z]+|[^\s\u0E00-\u0E7F\da-zA-Z]', text)
        return [token for token in tokens if token.strip()]
    
    def extract_time_patterns(self, text: str) -> List[Dict]:
        """Extract time patterns using comprehensive rules"""
        matches = []
        import re
        
        # 1. Formal time patterns (24-hour)
        for pattern in self.time_patterns['formal']:
            for match in re.finditer(pattern, text):
                hour = int(match.group('h'))
                minute = int(match.group('m')) if 'm' in match.groupdict() and match.group('m') else 0
                
                matches.append({
                    'type': 'time',
                    'category': 'formal',
                    'text': match.group(),
                    'hour': hour,
                    'minute': minute,
                    'start': match.start(),
                    'end': match.end()
                })
        
        # 2. Spoken time patterns
        for pattern in self.time_patterns['spoken']:
            for match in re.finditer(pattern, text):
                time_info = self.parse_spoken_time(match)
                if time_info:
                    matches.append({
                        'type': 'time',
                        'category': 'spoken',
                        'text': match.group(),
                        'hour': time_info['hour'],
                        'minute': time_info['minute'],
                        'start': match.start(),
                        'end': match.end()
                    })
        
        # 3. Special time expressions
        for expr, (hour, minute) in self.time_mappings['expressions'].items():
            if expr in text:
                pos = text.find(expr)
                matches.append({
                    'type': 'time',
                    'category': 'expression',
                    'text': expr,
                    'hour': hour,
                    'minute': minute,
                    'start': pos,
                    'end': pos + len(expr)
                })
        
        return matches
    
    def parse_spoken_time(self, match) -> Dict:
        """Parse spoken time patterns"""
        text = match.group().lower()
        
        # Handle special cases first
        if '‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏á‡∏ï‡∏£‡∏á' in text or text == '‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏á':
            return {'hour': 12, 'minute': 0}
        elif '‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏á‡∏Ñ‡∏∑‡∏ô' in text:
            return {'hour': 0, 'minute': 0}
        
        import re
        
        # Extract hour if present
        hour_match = re.search(r'(\d{1,2})', text)
        hour = int(hour_match.group(1)) if hour_match else None
        
        # Check for half hour
        minute = 30 if '‡∏Ñ‡∏£‡∏∂‡πà‡∏á' in text else 0
        
        if hour is None:
            # Period-only patterns (‡∏ï‡∏≠‡∏ô‡πÄ‡∏ä‡πâ‡∏≤, etc.)
            for period, (start_h, end_h) in self.time_patterns['period_defaults'].items():
                if period in text:
                    return {'hour': start_h, 'minute': 0}  # Use start of period as default
            return None
        
        # Apply period-specific adjustments - FIXED LOGIC
        if '‡∏ö‡πà‡∏≤‡∏¢' in text or '‡∏ö‡πà‡∏≤‡∏¢' in match.string[max(0, match.start()-10):match.end()+10]:
            # "‡∏ö‡πà‡∏≤‡∏¢ 3 ‡πÇ‡∏°‡∏á" or "‡∏ö‡πà‡∏≤‡∏¢3‡πÇ‡∏°‡∏á" -> 15:00
            if hour <= 12:
                hour = hour + 12
        elif '‡πÄ‡∏¢‡πá‡∏ô' in text:
            # "4 ‡πÇ‡∏°‡∏á‡πÄ‡∏¢‡πá‡∏ô" -> 16:00
            if hour <= 7:
                hour = hour + 12
        elif '‡∏ó‡∏∏‡πà‡∏°' in text:
            # "‡∏™‡∏≤‡∏°‡∏ó‡∏∏‡πà‡∏°" -> 21:00 (3 + 18)
            hour = hour + 18
        elif '‡πÄ‡∏ä‡πâ‡∏≤' in text:
            # Morning hours - keep as is if reasonable
            if hour > 12:
                hour = hour - 12
        
        # Validate hour range
        if hour >= 24:
            hour = hour - 24
        elif hour < 0:
            hour = hour + 24
        
        return {'hour': hour, 'minute': minute}
    
    def extract_date_patterns(self, text: str) -> List[Dict]:
        """Extract date patterns"""
        matches = []
        
        # Relative days
        for day_text, offset in self.date_patterns['relative_day'].items():
            if day_text in text:
                pos = text.find(day_text)
                matches.append({
                    'type': 'date',
                    'category': 'relative',
                    'text': day_text,
                    'offset_days': offset,
                    'start': pos,
                    'end': pos + len(day_text)
                })
        
        # Weekdays with modifiers
        import re
        weekday_pattern = r'(?:‡∏ß‡∏±‡∏ô)?(‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå|‡∏≠‡∏±‡∏á‡∏Ñ‡∏≤‡∏£|‡∏û‡∏∏‡∏ò|‡∏û‡∏§‡∏´‡∏±‡∏™‡∏ö‡∏î‡∏µ|‡∏û‡∏§‡∏´‡∏±‡∏™|‡∏®‡∏∏‡∏Å‡∏£‡πå|‡πÄ‡∏™‡∏≤‡∏£‡πå|‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå)(?:(‡∏ô‡∏µ‡πâ|‡∏´‡∏ô‡πâ‡∏≤|‡∏ñ‡∏±‡∏î‡πÑ‡∏õ|‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß))?'
        for match in re.finditer(weekday_pattern, text):
            weekday = match.group(1)
            modifier = match.group(2)  # Don't default to '‡∏´‡∏ô‡πâ‡∏≤' - let it be None
            
            matches.append({
                'type': 'date',
                'category': 'weekday',
                'text': match.group(),
                'weekday': weekday,
                'modifier': modifier,
                'start': match.start(),
                'end': match.end()
            })

        # --- Thai explicit date patterns (Integrated) ---
        # Extended to include informal forms (‡∏°‡∏Å‡∏£‡∏≤ ‡∏Å‡∏∏‡∏°‡∏†‡∏≤ ‡∏°‡∏µ‡∏ô‡∏≤ ‡πÄ‡∏°‡∏©‡∏≤ ‡∏û‡∏§‡∏©‡∏†‡∏≤ ‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤ ‡∏Å‡∏£‡∏Å‡∏é‡∏≤ ‡∏™‡∏¥‡∏á‡∏´‡∏≤ ‡∏Å‡∏±‡∏ô‡∏¢‡∏≤ ‡∏ï‡∏∏‡∏•‡∏≤ ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤ ‡∏ò‡∏±‡∏ô‡∏ß‡∏≤)
        # Negative lookahead to avoid treating time hour (e.g. 15:30) as year
        thai_date_regex = (
            r'(?:‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà\s*)?(\d{1,2})\s+'
            r'(‡∏°\.‡∏Ñ\.|‡∏Å\.‡∏û\.|‡∏°‡∏µ\.‡∏Ñ\.|‡πÄ‡∏°\.‡∏¢\.|‡∏û\.‡∏Ñ\.|‡∏°‡∏¥\.‡∏¢\.|‡∏Å\.‡∏Ñ\.|‡∏™\.‡∏Ñ\.|‡∏Å\.‡∏¢\.|‡∏ï\.‡∏Ñ\.|‡∏û\.‡∏¢\.|‡∏ò\.‡∏Ñ\.|'
            r'‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå|‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°|‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô|‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°|‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô|‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°|‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô|‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°|‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô|‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°|'
            r'‡∏°‡∏Ñ|‡∏Å‡∏û|‡∏°‡∏µ‡∏Ñ|‡πÄ‡∏°‡∏¢|‡∏û‡∏Ñ|‡∏°‡∏¥‡∏¢|‡∏Å‡∏Ñ|‡∏™‡∏Ñ|‡∏Å‡∏¢|‡∏ï‡∏Ñ|‡∏û‡∏¢|‡∏ò‡∏Ñ|'
            r'‡∏°‡∏Å‡∏£‡∏≤|‡∏Å‡∏∏‡∏°‡∏†‡∏≤|‡∏°‡∏µ‡∏ô‡∏≤|‡πÄ‡∏°‡∏©‡∏≤|‡∏û‡∏§‡∏©‡∏†‡∏≤|‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤|‡∏Å‡∏£‡∏Å‡∏é‡∏≤|‡∏™‡∏¥‡∏á‡∏´‡∏≤|‡∏Å‡∏±‡∏ô‡∏¢‡∏≤|‡∏ï‡∏∏‡∏•‡∏≤|‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤|‡∏ò‡∏±‡∏ô‡∏ß‡∏≤)'
            r'(?:\s*(\d{2,4})(?![:\.]))?'
        )
        for m in re.finditer(thai_date_regex, text):
            day_txt, month_txt, year_txt = m.groups()
            try:
                day = int(day_txt)
            except ValueError:
                continue
            month = getattr(self, 'thai_months', {}).get(month_txt)
            if not month:
                continue
            from datetime import datetime as _dt
            now = _dt.now()
            if year_txt:
                year = int(year_txt)
                # Convert 2-digit year
                if year < 100:
                    year += 2000
                # Convert Buddhist Era (BE) to Gregorian if clearly in BE range
                if year >= 2400:  # e.g. 2569
                    year -= 543
            else:
                year = now.year
                if (month < now.month) or (month == now.month and day < now.day):
                    year += 1
            import datetime as _d
            try:
                _d.date(year, month, day)
            except ValueError:
                continue
            matches.append({
                'type': 'date',
                'category': 'thai_date',
                'text': m.group(),
                'day': day,
                'month': month,
                'year': year,
                'start': m.start(),
                'end': m.end()
            })
        
        return matches
    
    def extract_location_patterns(self, text: str) -> List[Dict]:
        """Extract location patterns using enhanced rules - improved to avoid contamination"""
        matches = []
        import re
        
        # Enhanced location patterns with "‡∏ó‡∏µ‡πà" prefix support (Thai character support)
        # More restrictive to avoid contamination
        general_location_patterns = [
            r'‡∏ó‡∏µ‡πà‡∏£‡πâ‡∏≤‡∏ô[\w\u0e00-\u0e7f]*(?=\s|$|‡∏Å‡∏±‡∏ö|[,.])',               # ‡∏ó‡∏µ‡πà‡∏£‡πâ‡∏≤‡∏ô (stop at ‡∏Å‡∏±‡∏ö, space, end, punctuation)
            r'‡∏ó‡∏µ‡πà‡∏ö‡πâ‡∏≤‡∏ô[\w\u0e00-\u0e7f]*(?=\s|$|‡∏Å‡∏±‡∏ö|[,.])',               # ‡∏ó‡∏µ‡πà‡∏ö‡πâ‡∏≤‡∏ô
            r'‡∏ó‡∏µ‡πà‡∏á‡∏≤‡∏ô[\w\u0e00-\u0e7f]*(?=\s|$|‡∏Å‡∏±‡∏ö|[,.])',                # ‡∏ó‡∏µ‡πà‡∏á‡∏≤‡∏ô
            r'‡∏ó‡∏µ‡πà‡∏™‡∏ß‡∏ô[\w\u0e00-\u0e7f]*(?=\s|$|‡∏Å‡∏±‡∏ö|[,.])',                # ‡∏ó‡∏µ‡πà‡∏™‡∏ß‡∏ô
            r'‡∏ó‡∏µ‡πà‡∏ï‡∏•‡∏≤‡∏î[\w\u0e00-\u0e7f]*(?=\s|$|‡∏Å‡∏±‡∏ö|[,.])',               # ‡∏ó‡∏µ‡πà‡∏ï‡∏•‡∏≤‡∏î
            r'‡∏ó‡∏µ‡πà‡∏™‡∏ô‡∏≤‡∏°[\w\u0e00-\u0e7f]*(?=\s|$|‡∏Å‡∏±‡∏ö|[,.])',               # ‡∏ó‡∏µ‡πà‡∏™‡∏ô‡∏≤‡∏°
            r'‡∏ó‡∏µ‡πà‡πÇ‡∏£‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô[\w\u0e00-\u0e7f]*(?=\s|$|‡∏Å‡∏±‡∏ö|[,.])',           # ‡∏ó‡∏µ‡πà‡πÇ‡∏£‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
            r'‡∏ó‡∏µ‡πà‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢[\w\u0e00-\u0e7f]*(?=\s|$|‡∏Å‡∏±‡∏ö|[,.])',        # ‡∏ó‡∏µ‡πà‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢
            r'‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡πÄ‡∏ü‡πà[\w\u0e00-\u0e7f]*(?=\s|$|‡∏Å‡∏±‡∏ö|[,.])',              # ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡πÄ‡∏ü‡πà
            r'‡∏ó‡∏µ‡πà‡∏´‡πâ‡∏≤‡∏á[\w\u0e00-\u0e7f]*(?=\s|$|‡∏Å‡∏±‡∏ö|[,.])',               # ‡∏ó‡∏µ‡πà‡∏´‡πâ‡∏≤‡∏á (stop before ‡∏Å‡∏±‡∏ö)
        ]
        
        # Check for general location patterns first
        for pattern in general_location_patterns:
            for match in re.finditer(pattern, text):
                location = match.group().strip()  # Get full match including "‡∏ó‡∏µ‡πà"
                if location and len(location) > 3:
                    matches.append({
                        'type': 'location',
                        'category': 'general_with_prefix',
                        'text': location,
                        'start': match.start(),
                        'end': match.end()
                    })
        
        # Connector-based locations (more flexible) - also add stop conditions
        for pattern in self.location_patterns['connectors']:
            # Modify pattern to add stop conditions
            modified_pattern = pattern.replace(')', r')(?=\s|$|‡∏Å‡∏±‡∏ö|[,.])')
            for match in re.finditer(modified_pattern, text):
                location = match.group(1).strip()
                if location and len(location) > 1:
                    matches.append({
                        'type': 'location',
                        'category': 'general',
                        'text': location,
                        'start': match.start(),
                        'end': match.end()
                    })
        
        # Look for location words without connectors (for cases like "‡πÄ‡∏ã‡πá‡∏ô‡∏ó‡∏£‡∏±‡∏•‡∏•‡∏≤‡∏î‡∏û‡∏£‡πâ‡∏≤‡∏ß")
        location_keywords = [
            r'(‡πÄ‡∏ã‡πá‡∏ô‡∏ó‡∏£‡∏±‡∏•[^\s]*)',
            r'(‡∏™‡∏¢‡∏≤‡∏°[^\s]*)',
            r'(‡πÄ‡∏≠‡πá‡∏°‡∏ö‡∏µ‡πÄ‡∏Ñ)',
            r'(MBK)',
            r'([^\s]*‡∏´‡πâ‡∏≤‡∏á[^\s]*)',
            r'([^\s]*‡∏û‡∏•‡∏≤‡∏ã‡πà‡∏≤[^\s]*)',
            r'([^\s]*‡∏°‡∏≠‡∏•‡∏•‡πå[^\s]*)'
        ]
        
        for pattern in location_keywords:
            for match in re.finditer(pattern, text):
                location = match.group(1).strip()
                if location and len(location) > 2:
                    matches.append({
                        'type': 'location',
                        'category': 'mall',
                        'text': location,
                        'start': match.start(),
                        'end': match.end()
                    })
        
        # Specific location patterns
        for pattern in self.location_patterns['specific']:
            for match in re.finditer(pattern, text):
                location = match.group(1).strip() if match.groups() else match.group().strip()
                if location:
                    category = 'hospital' if any(word in pattern for word in ['‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•', '‡∏£‡∏û', '‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å']) else 'specific'
                    matches.append({
                        'type': 'location', 
                        'category': category,
                        'text': location,
                        'start': match.start(),
                        'end': match.end()
                    })

        # --- English location detection (added) ---
        # 1) Connector-based: at/in + Capitalized phrase (1-5 tokens)
        english_conn_regex = r'\b(?:at|in)\s+([A-Z][A-Za-z0-9&\-]*(?:\s+[A-Z][A-Za-z0-9&\-]*){0,4})'
        for m in re.finditer(english_conn_regex, text):
            phrase = m.group(1).strip()
            # Basic filtering: ignore if looks like a time or too short
            if len(phrase) < 3:
                continue
            if re.match(r'^[A-Z]\d+$', phrase):  # e.g., A1
                continue
            # Avoid duplicates (overlap with existing matches)
            if any(abs(m.start() - ex['start']) < 2 and ex['text'] == phrase for ex in matches):
                continue
            matches.append({
                'type': 'location',
                'category': 'english',
                'text': phrase,
                'start': m.start(),
                'end': m.end()
            })

        # 2) Known English location names without connector
        known_english_places = [
            'CentralWorld', 'Central Plaza', 'Siam Paragon', 'MBK', 'Terminal 21',
            'Bangkok Hospital', 'Samitivej', 'Samitivej Hospital', 'ICONSIAM',
            'EmQuartier', 'Emporium', 'Union Mall', 'Suvarnabhumi', 'Don Mueang', 'Don Muang',
            'Mega Bangna', 'Future Park'
        ]
        # Build regex that tolerates optional spaces in multi-word names
        place_pattern = r'(' + '|'.join([re.escape(p) for p in known_english_places]) + r')'
        for m in re.finditer(place_pattern, text):
            phrase = m.group(1).strip()
            if not any(m.start() == ex['start'] and ex['text'] == phrase for ex in matches):
                matches.append({
                    'type': 'location',
                    'category': 'english_known',
                    'text': phrase,
                    'start': m.start(),
                    'end': m.end()
                })
        
        return matches
    
    def extract_contact_patterns(self, text: str) -> List[Dict]:
        """Extract contact patterns"""
        matches = []
        import re
        
        # Phone patterns
        phone_patterns = [
            r'(\+66\s?\d{1,2}\s?\d{3,4}\s?\d{4})',
            r'(0\d{2}-?\d{3}-?\d{4})',
            r'(0\d{1}-?\d{3}-?\d{4})',
            r'(\d{3}-?\d{3}-?\d{4})'
        ]
        
        for pattern in phone_patterns:
            for match in re.finditer(pattern, text):
                matches.append({
                    'type': 'contact',
                    'category': 'phone',
                    'text': match.group(1),
                    'start': match.start(),
                    'end': match.end()
                })
        
        return matches
    
    def preprocess_text_advanced(self, text: str) -> Dict[str, Any]:
        """
        Advanced text preprocessing with PyThaiNLP
        (‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏™‡πà PyThaiNLP)
        """
        if not self.use_pythainlp:
            return self.preprocess_text_basic(text)
        
        # ‡∏à‡∏∞‡πÉ‡∏™‡πà PyThaiNLP logic ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
        """
        # 1. Normalize text
        normalized = normalize(text)
        
        # 2. Word tokenization
        tokens = word_tokenize(normalized, engine='newmm')
        
        # 3. POS tagging
        pos_tags = pos_tag(tokens, engine='perceptron')
        
        # 4. Named entity recognition
        entities = named_entity(normalized)
        
        # 5. Extract stopwords
        stopwords = thai_stopwords()
        filtered_tokens = [token for token in tokens if token not in stopwords]
        
        return {
            'original': text,
            'normalized': normalized,
            'tokens': tokens,
            'filtered_tokens': filtered_tokens,
            'pos_tags': pos_tags,
            'entities': entities,
            'context': self.detect_advanced_context(pos_tags, entities)
        }
        """
        pass
    
    def convert_thai_numerals(self, text: str) -> str:
        """‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏•‡∏Ç‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≤‡∏£‡∏ö‡∏¥‡∏Å"""
        mapping = getattr(self, 'thai_numerals', None)
        if mapping is None:
            mapping = self.normalization_maps.get('thai_numerals', {})
        for thai, arabic in mapping.items():
            text = text.replace(thai, arabic)
        return text
    
    def extract_basic_patterns(self, text: str, pattern_type: str) -> List[Dict]:
        """Extract patterns ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ PyThaiNLP"""
        matches = []
        
        if pattern_type == 'time':
            patterns = self.time_patterns
        elif pattern_type == 'location':
            patterns = self.location_patterns
        elif pattern_type == 'contact':
            patterns = self.contact_patterns
        else:
            return matches
        
        for category, pattern_list in patterns.items():
            for pattern in pattern_list:
                for match in re.finditer(pattern, text):
                    matches.append({
                        'type': pattern_type,
                        'category': category,
                        'text': match.group(),
                        'start': match.start(),
                        'end': match.end()
                    })
        
        return matches
    
    def detect_basic_context(self, text: str) -> Dict[str, Any]:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÅ‡∏ö‡∏ö basic"""
        context = {
            'type': 'general',
            'medical_score': 0,
            'business_score': 0,
            'urgency': 'normal',
            'formality': 'neutral'
        }
        
        # Check medical context
        for category, keywords in self.medical_keywords.items():
            for keyword in keywords:
                if keyword in text:
                    context['medical_score'] += 1
        
        # Check business context
        for category, keywords in self.business_keywords.items():
            for keyword in keywords:
                if keyword in text:
                    context['business_score'] += 1
        
        # Determine primary context
        if context['medical_score'] > context['business_score']:
            context['type'] = 'medical'
        elif context['business_score'] > context['medical_score']:
            context['type'] = 'business'
        
        return context
    
    def extract_smart_datetime(self, processed_text: Dict) -> Optional[datetime]:
        """
        Extract datetime using enhanced rules
        """
        from datetime import datetime, timedelta
        import calendar
        
        # Get matches
        time_matches = processed_text.get('time_matches', [])
        date_matches = processed_text.get('date_matches', [])
        
        # Start with current date/time
        now = datetime.now()
        target_date = now.date()
        target_time = None
        
        # Process date matches
        for date_match in date_matches:
            if date_match.get('category') == 'thai_date':
                try:
                    target_date = datetime(
                        date_match['year'],
                        date_match['month'],
                        date_match['day']
                    ).date()
                    # Explicit date found; stop processing further date matches
                    break
                except Exception:
                    pass
            if date_match['category'] == 'relative':
                # ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ, ‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ, etc.
                offset = date_match['offset_days']
                target_date = now.date() + timedelta(days=offset)
                
            elif date_match['category'] == 'weekday':
                # ‡∏ß‡∏±‡∏ô‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå‡∏´‡∏ô‡πâ‡∏≤, etc.
                weekday_name = date_match['weekday']
                modifier = date_match['modifier']
                
                # Map weekday name to number
                weekday_map = self.date_patterns['weekday']
                if weekday_name in weekday_map:
                    target_weekday = weekday_map[weekday_name]
                    
                    # For Thai context: when no modifier specified, find next occurrence
                    if modifier == '‡∏´‡∏ô‡πâ‡∏≤':
                        week_offset = 1  # Next week explicitly
                    elif modifier == '‡∏ô‡∏µ‡πâ':
                        week_offset = 0  # This week
                    elif modifier is None:
                        # No modifier - find next occurrence of this weekday
                        week_offset = 0  # Use logic in _calculate_weekday_date to find next occurrence
                    else:
                        week_offset = 0  # Default to current logic
                    
                    # Calculate target date
                    target_date = self._calculate_weekday_date(now.date(), target_weekday, week_offset)
        
        # Process time matches
        for time_match in time_matches:
            hour = time_match.get('hour')
            minute = time_match.get('minute', 0)
            
            if hour is not None:
                target_time = (hour, minute)
                break  # Use first valid time match
        
        # Default time if no time specified
        if target_time is None:
            target_time = (9, 0)  # Default to 9:00 AM
        
        # Combine date and time
        try:
            result_datetime = datetime.combine(target_date, datetime.min.time().replace(
                hour=target_time[0], minute=target_time[1]
            ))
            
            # Post-validate: don't allow past dates (adjust to next week if needed)
            if result_datetime < now:
                if result_datetime.date() == now.date():
                    # Same day but past time - keep it (might be intentional)
                    pass
                else:
                    # Past date - move to next week
                    result_datetime += timedelta(weeks=1)
            
            return result_datetime
            
        except ValueError:
            # Invalid datetime combination
            return None
    
    def _calculate_weekday_date(self, base_date, target_weekday: int, week_offset: int = 0):
        """Calculate date for specific weekday"""
        from datetime import timedelta
        
        # Current weekday (0=Monday, 6=Sunday)
        current_weekday = base_date.weekday()
        
        # Days until target weekday
        days_ahead = target_weekday - current_weekday
        
        # Adjust for week offset
        if week_offset == 0:  # This week
            if days_ahead <= 0:  # Target day already passed this week
                days_ahead += 7  # Move to next week
        elif week_offset == 1:  # Next week
            if days_ahead <= 0:
                days_ahead += 7  # Next occurrence
            # For Thai context: when saying "‡∏û‡∏§‡∏´‡∏±‡∏™" (Thursday), usually mean next Thursday
            # So if today is Monday and we say "‡∏û‡∏§‡∏´‡∏±‡∏™", we mean this Thursday (not next week)
            # But if today is Friday, we mean next Thursday
            pass  # Keep days_ahead as calculated
        elif week_offset == -1:  # Last week
            days_ahead -= 7
        
        return base_date + timedelta(days=days_ahead)
    
    def _end_of_month(self, date):
        """Get end of month date"""
        from datetime import datetime
        import calendar
        last_day = calendar.monthrange(date.year, date.month)[1]
        return date.replace(day=last_day)
    
    def _start_of_month(self, date):
        """Get start of month date"""
        return date.replace(day=1)
    
    def _next_month(self, date):
        """Get date in next month"""
        from datetime import datetime
        if date.month == 12:
            return date.replace(year=date.year + 1, month=1)
        else:
            return date.replace(month=date.month + 1)
    
    def extract_smart_location(self, processed_text: Dict) -> str:
        """
        Extract location ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏â‡∏•‡∏≤‡∏î ‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á location ‡πÅ‡∏•‡∏∞ building_dept
        ‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏°‡∏≤‡∏õ‡∏ô
        """
        location_matches = processed_text.get('location_matches', [])
        text = processed_text.get('normalized', '')
        
        # First, check if we already found building/department info
        building_dept = self.extract_building_department(processed_text)
        
        # First try to get from location matches
        if location_matches:
            context = processed_text.get('context', {})
            if context.get('type') == 'medical':
                for match in location_matches:
                    if match['category'] == 'hospital':
                        return self.clean_location_text(match['text'])
            
            # Return first general location that's not building-specific
            for match in location_matches:
                location = match['text']
                # Skip very short or obviously wrong locations
                if len(location) > 2 and not any(word in location.lower() for word in ['‡∏ö‡πà‡∏≤‡∏¢', '‡πÄ‡∏ä‡πâ‡∏≤', '‡πÇ‡∏°‡∏á']):
                    # If we have building_dept, skip building-specific terms in location
                    if building_dept:
                        building_terms = ['‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£', '‡∏ä‡∏±‡πâ‡∏ô', '‡πÅ‡∏ú‡∏ô‡∏Å', '‡∏´‡πâ‡∏≠‡∏á', '‡∏ï‡∏∂‡∏Å', '‡πÇ‡∏ã‡∏ô', '‡∏ù‡πà‡∏≤‡∏¢', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô', '‡∏®‡∏π‡∏ô‡∏¢‡πå']
                        if not any(term in location for term in building_terms):
                            return self.clean_location_text(location)
                    else:
                        return self.clean_location_text(location)
        
        # If no good location found from matches, try manual extraction
        # Look for "‡∏ó‡∏µ‡πà" + general places (not building-specific)
        import re
        general_place_patterns = [
            r'‡∏ó‡∏µ‡πà‡∏£‡πâ‡∏≤‡∏ô[\w\u0e00-\u0e7f]*',               # ‡∏ó‡∏µ‡πà‡∏£‡πâ‡∏≤‡∏ô, ‡∏ó‡∏µ‡πà‡∏£‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡πÅ‡∏ü
            r'‡∏ó‡∏µ‡πà‡∏ö‡πâ‡∏≤‡∏ô[\w\u0e00-\u0e7f]*',               # ‡∏ó‡∏µ‡πà‡∏ö‡πâ‡∏≤‡∏ô, ‡∏ó‡∏µ‡πà‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô
            r'‡∏ó‡∏µ‡πà‡∏á‡∏≤‡∏ô[\w\u0e00-\u0e7f]*',                # ‡∏ó‡∏µ‡πà‡∏á‡∏≤‡∏ô, ‡∏ó‡∏µ‡πà‡∏á‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡∏á
            r'‡∏ó‡∏µ‡πà‡∏™‡∏ß‡∏ô[\w\u0e00-\u0e7f]*',                # ‡∏ó‡∏µ‡πà‡∏™‡∏ß‡∏ô, ‡∏ó‡∏µ‡πà‡∏™‡∏ß‡∏ô‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞
            r'‡∏ó‡∏µ‡πà‡∏ï‡∏•‡∏≤‡∏î[\w\u0e00-\u0e7f]*',               # ‡∏ó‡∏µ‡πà‡∏ï‡∏•‡∏≤‡∏î, ‡∏ó‡∏µ‡πà‡∏ï‡∏•‡∏≤‡∏î‡∏ô‡∏±‡∏î
            r'‡∏ó‡∏µ‡πà‡∏™‡∏ô‡∏≤‡∏°[\w\u0e00-\u0e7f]*',               # ‡∏ó‡∏µ‡πà‡∏™‡∏ô‡∏≤‡∏°, ‡∏ó‡∏µ‡πà‡∏™‡∏ô‡∏≤‡∏°‡∏Å‡∏µ‡∏¨‡∏≤
            r'‡∏ó‡∏µ‡πà‡πÇ‡∏£‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô[\w\u0e00-\u0e7f]*',           # ‡∏ó‡∏µ‡πà‡πÇ‡∏£‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
            r'‡∏ó‡∏µ‡πà‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢[\w\u0e00-\u0e7f]*',        # ‡∏ó‡∏µ‡πà‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢
            r'‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡πÄ‡∏ü‡πà[\w\u0e00-\u0e7f]*',              # ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡πÄ‡∏ü‡πà
        ]
        
        for pattern in general_place_patterns:
            match = re.search(pattern, text)
            if match:
                return self.clean_location_text(match.group().strip())
        
        return ""
    
    def clean_location_text(self, location_text: str) -> str:
        """Clean location text from time and person contamination"""
        import re
        
        # Remove time-related suffixes
        cleaned = re.sub(r'(‡∏ö‡πà‡∏≤‡∏¢|‡πÄ‡∏ä‡πâ‡∏≤|‡πÄ‡∏¢‡πá‡∏ô|‡∏Ñ‡πà‡∏≥|\d{1,2}[:.]?\d{0,2}|‡πÇ‡∏°‡∏á|‡∏ô‡∏≤‡∏ó‡∏µ|‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤).*$', '', location_text).strip()
        
        # Remove person indicators - if "‡∏Å‡∏±‡∏ö" appears, everything after it is person
        if '‡∏Å‡∏±‡∏ö' in cleaned:
            cleaned = cleaned.split('‡∏Å‡∏±‡∏ö')[0].strip()
        
        # Remove weekday references
        cleaned = re.sub(r'(‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå|‡∏≠‡∏±‡∏á‡∏Ñ‡∏≤‡∏£|‡∏û‡∏∏‡∏ò|‡∏û‡∏§‡∏´‡∏±‡∏™|‡∏û‡∏§‡∏´‡∏±‡∏™‡∏ö‡∏î‡∏µ|‡∏®‡∏∏‡∏Å‡∏£‡πå|‡πÄ‡∏™‡∏≤‡∏£‡πå|‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå).*$', '', cleaned).strip()
        
        return cleaned
    
    def extract_appointment_info(self, text: str) -> Dict[str, Any]:
        """
        Main method ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö extract ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏±‡∏î‡∏´‡∏°‡∏≤‡∏¢
        """
        # Preprocess text
        processed = self.preprocess_text_basic(text)  # ‡∏´‡∏£‡∏∑‡∏≠ advanced ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏û‡∏£‡πâ‡∏≠‡∏°
        
        # Extract components
        appointment_datetime = self.extract_smart_datetime(processed)
        location = self.extract_smart_location(processed)
        context = processed.get('context', {})
        
        # Extract appointment title
        appointment_title = self.extract_appointment_title(processed)
        
        # Extract contact person
        contact_person = self.extract_contact_person(processed)
        
        # Extract phone number
        phone_number = self.extract_phone_number(processed)
        
        # Extract building/department/floor
        building_dept = self.extract_building_department(processed)
        
        return {
            'original_text': text,
            'appointment_title': appointment_title,
            'datetime': appointment_datetime,
            'date': appointment_datetime.strftime('%d/%m/%Y') if appointment_datetime else '',
            'time': appointment_datetime.strftime('%H.%M') if appointment_datetime else '',
            'location': location,
            'building_dept': building_dept,
            'contact_person': contact_person,
            'phone_number': phone_number,
            'context': context,
            'confidence': self.calculate_confidence(processed),
            'processed_data': processed  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö debug
        }
    
    def extract_appointment_title(self, processed_text: Dict) -> str:
        """Extract appointment title/activity (clean of date/time/location)."""
        import re
        text = processed_text.get('normalized', '')

        # 1. Remove command prefixes
        for command in self.command_patterns['add_appointment']:
            text = text.replace(command, ' ').strip()

        activity_text = text

        # 2. Remove explicit time patterns
        time_removal_patterns = [
            r'\b\d{1,2}[:.]\d{2}\b\s*(‡∏ô\.|‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤)?',
            r'\b\d{1,2}\s*‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤\b',
            r'\b\d{1,2}\s*‡πÇ‡∏°‡∏á(?:‡πÄ‡∏ä‡πâ‡∏≤|‡πÄ‡∏¢‡πá‡∏ô|‡∏ï‡∏£‡∏á)?',
            r'\b\d{1,2}\s*‡∏ó‡∏∏‡πà‡∏°(?:‡∏Ñ‡∏£‡∏∂‡πà‡∏á)?',
            r'(?:‡∏ï‡∏≠‡∏ô)?(?:‡πÄ‡∏ä‡πâ‡∏≤|‡∏ö‡πà‡∏≤‡∏¢|‡πÄ‡∏¢‡πá‡∏ô|‡∏Ñ‡πà‡∏≥|‡∏Å‡∏•‡∏≤‡∏á‡∏ß‡∏±‡∏ô|‡∏Å‡∏•‡∏≤‡∏á‡∏Ñ‡∏∑‡∏ô)\s*\d*',
        ]
        for pattern in time_removal_patterns:
            activity_text = re.sub(pattern, ' ', activity_text)

        # 3. Remove weekdays & relative words
        activity_text = re.sub(r'(?:‡∏ß‡∏±‡∏ô)?(?:‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå|‡∏≠‡∏±‡∏á‡∏Ñ‡∏≤‡∏£|‡∏û‡∏∏‡∏ò|‡∏û‡∏§‡∏´‡∏±‡∏™‡∏ö‡∏î‡∏µ|‡∏û‡∏§‡∏´‡∏±‡∏™|‡∏®‡∏∏‡∏Å‡∏£‡πå|‡πÄ‡∏™‡∏≤‡∏£‡πå|‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå)(?:‡∏ô‡∏µ‡πâ|‡∏´‡∏ô‡πâ‡∏≤|‡∏ñ‡∏±‡∏î‡πÑ‡∏õ|‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß)?', ' ', activity_text)
        activity_text = re.sub(r'(‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ|‡∏°‡∏∞‡∏£‡∏∑‡∏ô‡∏ô‡∏µ‡πâ|‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ|‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô|‡∏õ‡∏µ‡∏´‡∏ô‡πâ‡∏≤|‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤|‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏´‡∏ô‡πâ‡∏≤)', ' ', activity_text)

        # 4. Remove parsed date substrings
        for dm in processed_text.get('date_matches', []):
            t = dm.get('text', '')
            if t:
                activity_text = activity_text.replace(t, ' ')
        activity_text = re.sub(r'‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', ' ', activity_text)

        # 5. Remove orphan word '‡πÄ‡∏ß‡∏•‡∏≤'
        activity_text = re.sub(r'\b‡πÄ‡∏ß‡∏•‡∏≤\b', ' ', activity_text)

        # 6. Remove contact marker + name (leave contact for separate field)
        activity_text = re.sub(r'‡∏Å‡∏±‡∏ö\S+', ' ', activity_text)

        # 7. Remove obvious trailing location phrases
        activity_text = re.sub(r'(?:‡∏ó‡∏µ‡πà|‡πÉ‡∏ô|‡∏ï‡πà‡∏≠|‡πÑ‡∏õ)(?:‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•|‡∏´‡πâ‡∏≤‡∏á|‡∏£‡πâ‡∏≤‡∏ô|‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ|‡∏ó‡∏µ‡πà|‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å).*$', ' ', activity_text)

        # 8. Remove detected locations (Thai/English)
        for loc in processed_text.get('location_matches', []):
            loc_text = loc.get('text', '')
            if loc_text:
                activity_text = activity_text.replace(loc_text, ' ')
        # English connectors
        activity_text = re.sub(r'\b(?:at|in|to)\b', ' ', activity_text, flags=re.IGNORECASE)

        # 9. Cleanup punctuation & spaces
        activity_text = re.sub(r'[,;.]+', ' ', activity_text)
        activity_text = re.sub(r'\s+', ' ', activity_text).strip()

        return activity_text if activity_text else '‡∏ô‡∏±‡∏î‡∏´‡∏°‡∏≤‡∏¢'
    
    def extract_contact_person(self, processed_text: Dict) -> str:
        """Extract contact person - improved to avoid time/location contamination"""
        text = processed_text.get('normalized', '')
        
        import re
        
        # Find all "‡∏Å‡∏±‡∏ö[person]" patterns with better boundaries
        contacts = []
        
        # Method 1: More precise pattern - stop at "‡∏ó‡∏µ‡πà", time words, or weekdays
        contact_pattern = r'‡∏Å‡∏±‡∏ö([^\s]+?)(?=\s*‡∏ó‡∏µ‡πà|‡∏ö‡πà‡∏≤‡∏¢|‡πÄ‡∏ä‡πâ‡∏≤|‡πÄ‡∏¢‡πá‡∏ô|‡∏Ñ‡πà‡∏≥|‡πÇ‡∏°‡∏á|‡∏ô‡∏≤‡∏ó‡∏µ|‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå|‡∏≠‡∏±‡∏á‡∏Ñ‡∏≤‡∏£|‡∏û‡∏∏‡∏ò|‡∏û‡∏§‡∏´‡∏±‡∏™|‡∏®‡∏∏‡∏Å‡∏£‡πå|‡πÄ‡∏™‡∏≤‡∏£‡πå|‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå|‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ|‡∏°‡∏∞‡∏£‡∏∑‡∏ô‡∏ô‡∏µ‡πâ|‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ|\d+|$)'
        
        matches = re.findall(contact_pattern, text)
        
        for contact in matches:
            # Clean up the contact name
            cleaned_contact = contact.strip()
            
            # Remove any remaining time/location indicators but be more conservative
            if any(word in cleaned_contact for word in ['‡∏ö‡πà‡∏≤‡∏¢', '‡πÄ‡∏ä‡πâ‡∏≤', '‡πÄ‡∏¢‡πá‡∏ô', '‡∏Ñ‡πà‡∏≥', '‡πÇ‡∏°‡∏á', '‡∏ô‡∏≤‡∏ó‡∏µ']):
                cleaned_contact = re.sub(r'(‡∏ö‡πà‡∏≤‡∏¢|‡πÄ‡∏ä‡πâ‡∏≤|‡πÄ‡∏¢‡πá‡∏ô|‡∏Ñ‡πà‡∏≥|‡πÇ‡∏°‡∏á|‡∏ô‡∏≤‡∏ó‡∏µ|‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤).*$', '', cleaned_contact).strip()
            
            if any(word in cleaned_contact for word in ['‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ', '‡∏°‡∏∞‡∏£‡∏∑‡∏ô‡∏ô‡∏µ‡πâ', '‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ', '‡∏´‡∏ô‡πâ‡∏≤', '‡∏ô‡∏µ‡πâ']):
                cleaned_contact = re.sub(r'(‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ|‡∏°‡∏∞‡∏£‡∏∑‡∏ô‡∏ô‡∏µ‡πâ|‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ|‡∏´‡∏ô‡πâ‡∏≤|‡∏ô‡∏µ‡πâ).*$', '', cleaned_contact).strip()
            
            # Only add if it's a reasonable length and doesn't contain obvious location/time words
            if cleaned_contact and len(cleaned_contact) > 0:
                # Skip if it contains obvious location/time words
                problematic_words = ['‡∏£‡πâ‡∏≤‡∏ô', '‡∏´‡πâ‡∏≤‡∏á', '‡∏ï‡∏∂‡∏Å', '‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£', '‡∏ä‡∏±‡πâ‡∏ô', '‡πÅ‡∏ú‡∏ô‡∏Å', '‡∏´‡πâ‡∏≠‡∏á', '‡∏™‡∏ô‡∏≤‡∏°', 
                                   '‡πÇ‡∏£‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô', '‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•', '‡πÇ‡∏°‡∏á', '‡∏ô‡∏≤‡∏ó‡∏µ']
                if not any(word in cleaned_contact for word in problematic_words):
                    contacts.append(cleaned_contact)
        
        # Method 2: If no contacts found, try a simpler pattern but clean more aggressively
        if not contacts:
            simple_pattern = r'‡∏Å‡∏±‡∏ö(\S+?)(?:\s|$)'
            simple_matches = re.findall(simple_pattern, text)
            
            for contact in simple_matches:
                cleaned_contact = contact.strip()
                
                # Stop at "‡∏ó‡∏µ‡πà" if present
                if '‡∏ó‡∏µ‡πà' in cleaned_contact:
                    cleaned_contact = cleaned_contact.split('‡∏ó‡∏µ‡πà')[0].strip()
                
                # Remove time/date suffixes
                cleaned_contact = re.sub(r'(‡∏ö‡πà‡∏≤‡∏¢|‡πÄ‡∏ä‡πâ‡∏≤|‡πÄ‡∏¢‡πá‡∏ô|‡∏Ñ‡πà‡∏≥|‡πÇ‡∏°‡∏á|‡∏ô‡∏≤‡∏ó‡∏µ|‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ|‡∏°‡∏∞‡∏£‡∏∑‡∏ô‡∏ô‡∏µ‡πâ|‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ|‡∏´‡∏ô‡πâ‡∏≤|‡∏ô‡∏µ‡πâ).*$', '', cleaned_contact).strip()
                
                if cleaned_contact and len(cleaned_contact) > 0:
                    # Less strict filtering for simple pattern
                    if not any(word in cleaned_contact for word in ['‡∏£‡πâ‡∏≤‡∏ô', '‡∏´‡πâ‡∏≤‡∏á', '‡∏ï‡∏∂‡∏Å', '‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£', '‡∏ä‡∏±‡πâ‡∏ô', '‡πÇ‡∏°‡∏á']):
                        contacts.append(cleaned_contact)
        
        # Return joined contacts or empty string
        return ', '.join(contacts) if contacts else ""
    
    def extract_phone_number(self, processed_text: Dict) -> str:
        """Extract phone number"""
        contact_matches = processed_text.get('contact_matches', [])
        
        for match in contact_matches:
            if match['category'] == 'phone':
                return match['text']
        
        return ""
    
    def extract_building_department(self, processed_text: Dict) -> str:
        """Extract building/department/floor info"""
        text = processed_text.get('normalized', '')
        
        import re
        
        # Enhanced patterns for building/department/floor - more specific
        building_patterns = [
            r'‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£[\w\u0e00-\u0e7f]*',               # ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£, ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£A (support Thai characters)
            r'‡∏ó‡∏µ‡πà‡∏ä‡∏±‡πâ‡∏ô\d+',                                # ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡πâ‡∏ô1, ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡πâ‡∏ô2
            r'‡∏ó‡∏µ‡πà‡πÅ‡∏ú‡∏ô‡∏Å[\w\u0e00-\u0e7f]+',               # ‡∏ó‡∏µ‡πà‡πÅ‡∏ú‡∏ô‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô (support Thai characters)
            r'‡∏ó‡∏µ‡πà‡∏´‡πâ‡∏≠‡∏á[\w\u0e00-\u0e7f]+',               # ‡∏ó‡∏µ‡πà‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° (support Thai characters)
            r'‡∏ó‡∏µ‡πà‡∏ï‡∏∂‡∏Å[\w\u0e00-\u0e7f]*',                # ‡∏ó‡∏µ‡πà‡∏ï‡∏∂‡∏Å, ‡∏ó‡∏µ‡πà‡∏ï‡∏∂‡∏ÅA (support Thai characters)
            r'‡∏ó‡∏µ‡πà‡πÇ‡∏ã‡∏ô[\w\u0e00-\u0e7f]*',                # ‡∏ó‡∏µ‡πà‡πÇ‡∏ã‡∏ôA (support Thai characters)
            r'‡∏ó‡∏µ‡πà‡∏ù‡πà‡∏≤‡∏¢[\w\u0e00-\u0e7f]+',               # ‡∏ó‡∏µ‡πà‡∏ù‡πà‡∏≤‡∏¢‡∏Ç‡∏≤‡∏¢ (support Thai characters)
            r'‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô[\w\u0e00-\u0e7f]*',           # ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô (support Thai characters)
            r'‡∏ó‡∏µ‡πà‡∏®‡∏π‡∏ô‡∏¢‡πå[\w\u0e00-\u0e7f]+',              # ‡∏ó‡∏µ‡πà‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° (support Thai characters)
            r'‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£[\w\u0e00-\u0e7f]*',                 # ‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£A (without ‡∏ó‡∏µ‡πà, support Thai characters)
            r'‡∏ä‡∏±‡πâ‡∏ô\d+',                                  # ‡∏ä‡∏±‡πâ‡∏ô1 (without ‡∏ó‡∏µ‡πà)
            r'‡πÅ‡∏ú‡∏ô‡∏Å[\w\u0e00-\u0e7f]+',                 # ‡πÅ‡∏ú‡∏ô‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô (without ‡∏ó‡∏µ‡πà, support Thai characters)
            r'‡∏´‡πâ‡∏≠‡∏á[\w\u0e00-\u0e7f]+',                 # ‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° (without ‡∏ó‡∏µ‡πà, support Thai characters)
        ]
        
        for pattern in building_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group().strip()
        
        return ""
    
    def calculate_confidence(self, processed_text: Dict) -> float:
        """
        Calculate confidence score based on field extraction and source quality
        """
        confidence = 0.0
        max_confidence = 1.0
        
        # Base confidence from found patterns
        time_matches = processed_text.get('time_matches', [])
        date_matches = processed_text.get('date_matches', [])
        location_matches = processed_text.get('location_matches', [])
        contact_matches = processed_text.get('contact_matches', [])
        
        # Time confidence (0.4 max)
        if time_matches:
            time_confidence = 0.0
            for match in time_matches:
                if match['category'] == 'formal':
                    time_confidence = max(time_confidence, 0.4)  # Highest confidence
                elif match['category'] == 'spoken':
                    time_confidence = max(time_confidence, 0.3)  # Good confidence
                elif match['category'] == 'expression':
                    time_confidence = max(time_confidence, 0.35) # Very good confidence
            confidence += time_confidence
        
        # Date confidence (0.25 max)
        if date_matches:
            date_confidence = 0.0
            for match in date_matches:
                if match['category'] == 'relative':
                    date_confidence = max(date_confidence, 0.25)  # Good confidence
                elif match['category'] == 'weekday':
                    date_confidence = max(date_confidence, 0.2)   # Decent confidence
            confidence += date_confidence
        
        # Location confidence (0.2 max)
        if location_matches:
            location_confidence = 0.0
            for match in location_matches:
                if match['category'] == 'specific':
                    location_confidence = max(location_confidence, 0.2)  # Highest for specific
                elif match['category'] == 'hospital':
                    location_confidence = max(location_confidence, 0.18) # High for medical
                else:
                    location_confidence = max(location_confidence, 0.15) # General location
            confidence += location_confidence
        
        # Contact confidence (0.15 max)
        if contact_matches:
            contact_confidence = 0.0
            for match in contact_matches:
                if match['category'] == 'phone':
                    contact_confidence = max(contact_confidence, 0.15)
            confidence += contact_confidence
        
        # Context understanding bonus (up to 0.1)
        context = processed_text.get('context', {})
        if context.get('type') != 'general':
            confidence += 0.05  # Bonus for understanding context
        
        # Field completeness bonus
        fields_found = 0
        if time_matches: fields_found += 1
        if date_matches: fields_found += 1  
        if location_matches: fields_found += 1
        if contact_matches: fields_found += 1
        
        # Bonus for multiple fields (shows comprehensive understanding)
        if fields_found >= 3:
            confidence += 0.1
        elif fields_found >= 2:
            confidence += 0.05
        
        return min(confidence, max_confidence)

# Example usage class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö testing
class ParserExample:
    """‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Enhanced Parser"""
    
    def __init__(self):
        self.parser = EnhancedSmartDateTimeParser()
    
    def test_basic_parsing(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö basic parsing"""
        test_cases = [
            "‡∏ô‡∏±‡∏î‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ 14.30 ‡∏ô. ‡∏ó‡∏µ‡πà‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡∏®‡∏¥‡∏£‡∏¥‡∏£‡∏≤‡∏ä",
            "‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡∏ß‡∏±‡∏ô‡∏û‡∏∏‡∏ò‡∏´‡∏ô‡πâ‡∏≤ 2 ‡πÇ‡∏°‡∏á‡πÄ‡∏¢‡πá‡∏ô ‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° A",
            "‡πÑ‡∏õ‡∏´‡∏≤‡∏´‡∏°‡∏≠‡∏™‡∏°‡∏ä‡∏≤‡∏¢ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ‡πë‡πï ‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå ‡πÄ‡∏ß‡∏•‡∏≤ ‡πë‡πê.‡πì‡πê ‡∏ô.",
            "‡∏ô‡∏±‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏ï‡∏≠‡∏ô‡πÄ‡∏ä‡πâ‡∏≤ ‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å‡∏´‡∏±‡∏ß‡πÉ‡∏à ‡∏£‡∏û.‡∏à‡∏∏‡∏¨‡∏≤"
        ]
        
        for text in test_cases:
            print(f"\nTesting: {text}")
            result = self.parser.extract_appointment_info(text)
            print(f"Result: {result}")
    
    def test_context_detection(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏ö‡∏ó"""
        contexts = [
            ("‡πÑ‡∏õ‡∏´‡∏≤‡∏´‡∏°‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏±‡∏ß‡πÉ‡∏à", "medical"),
            ("‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£", "business"),
            ("‡πÄ‡∏à‡∏≠‡∏Å‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏´‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏û‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", "general")
        ]
        
        for text, expected in contexts:
            processed = self.parser.preprocess_text_basic(text)
            actual = processed['context']['type']
            print(f"{text} -> {actual} (expected: {expected})")

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö testing
def run_examples():
    """‡∏£‡∏±‡∏ô examples ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
    example = ParserExample()
    
    print("=== Testing Basic Parsing ===")
    example.test_basic_parsing()
    
    print("\n=== Testing Context Detection ===")
    example.test_context_detection()

if __name__ == "__main__":
    run_examples()
